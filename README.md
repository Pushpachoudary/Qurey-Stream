# ğŸ“š Qurey-Stream


Upload any text or PDF file and ask questions about it â€” get accurate, AI-generated answers using Retrieval-Augmented Generation (RAG) with **local models** via Ollama.

No cloud, no API keys â€” 100% private and local.

---

## ğŸ§  What is this?

A private AI assistant for your documents!

- Upload a document (TXT or PDF)
- App splits and indexes it into a vector store using `ChromaDB`
- When you ask a question, it retrieves relevant chunks using semantic similarity
- Passes those chunks as context to a local LLM via Ollama (like `mistral`)
- You get precise answers from **your own data** â€” privately and offline!

---

## ğŸ–¼ï¸ UI Preview



## ğŸ§° Tech Stack

| ğŸ› ï¸ Tool      | ğŸ”§ Role                              |
|--------------|--------------------------------------|
| Python       | Core backend                         |
| Streamlit    | Frontend interface                   |
| LangChain    | Document splitting & retrieval       |
| ChromaDB     | Vector database                      |
| Ollama       | Local LLM & embeddings               |
| llama3.2:3b  | Default local model (can be changed) |

---

## âœ¨ Features

- ğŸ“„ Upload PDFs or TXT files  
- ğŸ” Ask questions and get relevant, contextual answers  
- ğŸ§  RAG-based workflow using local embeddings & models  
- ğŸ¨ Colorful UI with purple and blue palette  
- ğŸ“¤ Export answers as `.txt` file  
- ğŸ”’ 100% private & runs fully offline

---

## âš™ï¸ How to Run Locally

### 1ï¸âƒ£ Prerequisites

- Python 3.10+
- Git
- Ollama installed â†’ https://ollama.com/download
- Model pulled locally â†’ `ollama pull mistral`

### 2ï¸âƒ£ Clone the Repo

```bash
git clone https://github.com/Pushpachoudary/RAG-model.git
cd RAG-model
```

### 3ï¸âƒ£ (Optional) Create Virtual Environment

```bash
python -m venv venv
venv\Scripts\activate     # On Windows
source venv/bin/activate  # On Mac/Linux
```

### 4ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 5ï¸âƒ£ Run the App

```bash
streamlit run app.py
```

> Make sure Ollama is running in the background and the model (like `mistral`) is pulled.

---

## ğŸ“¤ Export Feature

After getting answers, use the **"Export Answer"** button to save the response to a `.txt` file locally.

---

## ğŸ“ Folder Structure

```bash
RAG-model/
â”‚
â”œâ”€â”€ app.py                # Main Streamlit app
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ README.md             # Project info
â””â”€â”€ .venv/                # (optional) virtual environment
```

---

## ğŸ“Œ Notes

- You can change the model from Mistral to any Ollama-supported LLM
- Supports only single document at a time (multi-doc feature coming soon)
- All data and models stay on your device â€” no uploads or APIs

---

## ğŸ‘¨â€ğŸ’» Author

**Pushpa Choudary**  
B.Tech CSE (AI & ML) Graduate  
[GitHub Profile Â»](https://github.com/Pushpachoudary)

---

## â­ï¸ Show your support

If you find this project helpful, consider giving it a â­ and sharing with others!

---
